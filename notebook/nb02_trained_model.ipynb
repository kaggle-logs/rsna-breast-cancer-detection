{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ktakeda/workspace/kaggle/rsna-breast-cancer-detection/rsna3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/ktakeda/workspace/kaggle/rsna-breast-cancer-detection\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pathlib \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local\n",
    "from rsna.utility import load_data, data_to_device, dicom2png\n",
    "from rsna.preprocess import Transform, df_preprocess\n",
    "from rsna.model import ResNet50Network, EfficientNet\n",
    "from rsna.config import DEVICE, PLATFORM\n",
    "from rsna.dataset import RSNADatasetPNG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルの評価\n",
    "\n",
    "- 2/7 時点で score 0.04 となった最新の学習済みモデルを評価する\n",
    "    - そもそも train dataset に対して、性能が出ているのか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (backbone): EfficientNet(\n",
       "    (conv_stem): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNormAct2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNormAct2d(\n",
       "      1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (dense1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (dense2): Linear(in_features=500, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset, dataloader\n",
    "df_train = load_data(\"train\", custom_path=\"/Users/ktakeda/workspace/kaggle/rsna-breast-cancer-detection/data/dicom2png_512\")\n",
    "df_train = df_preprocess(df_train, is_train=True, sampling=\"down\")\n",
    "df_train_0 = df_train[df_train[\"cancer\"]==0].reset_index(drop=True)\n",
    "df_train_1 = df_train[df_train[\"cancer\"]==1].reset_index(drop=True)\n",
    "transform = Transform(cfg=None, only_test=True) \n",
    "dataset0 = RSNADatasetPNG(df_train_0, transform.get(is_train=False), csv_columns = [\"laterality\", \"view\", \"age\", \"implant\"], has_target=True)\n",
    "dataset1 = RSNADatasetPNG(df_train_1, transform.get(is_train=False), csv_columns = [\"laterality\", \"view\", \"age\", \"implant\"], has_target=True)\n",
    "loader0 = DataLoader(dataset0, batch_size=8, shuffle=False, num_workers=2)\n",
    "loader1 = DataLoader(dataset1, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# load trained model\n",
    "model = EfficientNet(pretrained=False).to(DEVICE)\n",
    "model.load_state_dict(torch.load(f\"model_fold1_epoch49_vacc0.506_vpfbeta0.417.pth\", map_location=torch.device(DEVICE)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [04:43<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predict_true0, predict_true1 = [], []\n",
    "\n",
    "#for idx, data in enumerate(dataset0):\n",
    "#    if idx > 200: break\n",
    "#    image = torch.unsqueeze(data[\"image\"],1)\n",
    "#    target = data[\"target\"]\n",
    "#    out = model(image, None)\n",
    "#    pred = torch.sigmoid(out)\n",
    "#\n",
    "#    predict_true0.append(pred.squeeze(1).cpu().detach().numpy()[0])\n",
    "\n",
    "\n",
    "for data in tqdm(loader1, total=len(loader1)):\n",
    "    # image = torch.unsqueeze(data[\"image\"],1)\n",
    "    target = data[\"target\"]\n",
    "    out = model(data[\"image\"], None)\n",
    "    pred = torch.sigmoid(out)\n",
    "    predict_true1.extend(list(pred.squeeze(1).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "# ax.hist(predict_true0, label=\"0\")\n",
    "ax.hist(predict_true1, label=\"1\")\n",
    "ax.set_xlim(0,1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 全ての予測値が0.3 のスコアを持っていて、全く学習できていないことが分かる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36012304,\n",
       " 0.3594461,\n",
       " 0.36131585,\n",
       " 0.36028007,\n",
       " 0.3585494,\n",
       " 0.3602014,\n",
       " 0.3608389,\n",
       " 0.3591086,\n",
       " 0.36138454,\n",
       " 0.36097914,\n",
       " 0.35911343,\n",
       " 0.35975385,\n",
       " 0.3595489,\n",
       " 0.3620737,\n",
       " 0.36163542,\n",
       " 0.36011574,\n",
       " 0.36073896,\n",
       " 0.36117858,\n",
       " 0.36152756,\n",
       " 0.36135396,\n",
       " 0.35996136,\n",
       " 0.36198005,\n",
       " 0.3609696,\n",
       " 0.36107808,\n",
       " 0.3617612,\n",
       " 0.35884085,\n",
       " 0.3594469,\n",
       " 0.36043364,\n",
       " 0.3612876,\n",
       " 0.35896087,\n",
       " 0.3607085,\n",
       " 0.35967797,\n",
       " 0.36172318,\n",
       " 0.36140713,\n",
       " 0.36267918,\n",
       " 0.3618276,\n",
       " 0.36035377,\n",
       " 0.36084944,\n",
       " 0.36121225,\n",
       " 0.36073202,\n",
       " 0.36018217,\n",
       " 0.3612426,\n",
       " 0.3596945,\n",
       " 0.36095026,\n",
       " 0.35977075,\n",
       " 0.3605256,\n",
       " 0.36178833,\n",
       " 0.3602691,\n",
       " 0.36026907,\n",
       " 0.361259,\n",
       " 0.36154836,\n",
       " 0.36116698,\n",
       " 0.36028123,\n",
       " 0.35951936,\n",
       " 0.36137617,\n",
       " 0.35874686,\n",
       " 0.3618238,\n",
       " 0.36185622,\n",
       " 0.35996702,\n",
       " 0.3612517,\n",
       " 0.36023423,\n",
       " 0.3608338,\n",
       " 0.36210948,\n",
       " 0.36201367,\n",
       " 0.36193258,\n",
       " 0.36155194,\n",
       " 0.36054152,\n",
       " 0.36025387,\n",
       " 0.3605476,\n",
       " 0.36035255,\n",
       " 0.3595505,\n",
       " 0.35756963,\n",
       " 0.35962954,\n",
       " 0.36030698,\n",
       " 0.3603796,\n",
       " 0.361304,\n",
       " 0.3617084,\n",
       " 0.35922942,\n",
       " 0.36221293,\n",
       " 0.35873333,\n",
       " 0.35961282,\n",
       " 0.36142403,\n",
       " 0.36009848,\n",
       " 0.35951918,\n",
       " 0.36249834,\n",
       " 0.3607026,\n",
       " 0.35935628,\n",
       " 0.36193836,\n",
       " 0.36061633,\n",
       " 0.36010888,\n",
       " 0.3613386,\n",
       " 0.3597812,\n",
       " 0.36161307,\n",
       " 0.3612396,\n",
       " 0.362313,\n",
       " 0.36210987,\n",
       " 0.36015585,\n",
       " 0.36072806,\n",
       " 0.3595942,\n",
       " 0.35951605,\n",
       " 0.36067495,\n",
       " 0.36016813,\n",
       " 0.36097324,\n",
       " 0.3601163,\n",
       " 0.36093286,\n",
       " 0.3602062,\n",
       " 0.3606941,\n",
       " 0.36175677,\n",
       " 0.3589933,\n",
       " 0.35890833,\n",
       " 0.35947943,\n",
       " 0.3606881,\n",
       " 0.36070094,\n",
       " 0.36144346,\n",
       " 0.3610309,\n",
       " 0.36090764,\n",
       " 0.35820574,\n",
       " 0.3607303,\n",
       " 0.35842264,\n",
       " 0.36030564,\n",
       " 0.35956228,\n",
       " 0.36043903,\n",
       " 0.3599753,\n",
       " 0.35992348,\n",
       " 0.360657,\n",
       " 0.36043572,\n",
       " 0.36049458,\n",
       " 0.36153385,\n",
       " 0.35911793,\n",
       " 0.36019337,\n",
       " 0.3608653,\n",
       " 0.3615498,\n",
       " 0.35984865,\n",
       " 0.3612468,\n",
       " 0.36115953,\n",
       " 0.36032084,\n",
       " 0.36073747,\n",
       " 0.36067867,\n",
       " 0.3571332,\n",
       " 0.3598224,\n",
       " 0.3601582,\n",
       " 0.3588428,\n",
       " 0.35829648,\n",
       " 0.35843098,\n",
       " 0.3591514,\n",
       " 0.3605298,\n",
       " 0.35926387,\n",
       " 0.36172324,\n",
       " 0.36210755,\n",
       " 0.3604135,\n",
       " 0.36015278,\n",
       " 0.36017406,\n",
       " 0.36066315,\n",
       " 0.3600154,\n",
       " 0.36032933,\n",
       " 0.36006132,\n",
       " 0.36033106,\n",
       " 0.36050424,\n",
       " 0.36007088,\n",
       " 0.36013865,\n",
       " 0.3603206,\n",
       " 0.36016247,\n",
       " 0.35973507,\n",
       " 0.36058035,\n",
       " 0.36103252,\n",
       " 0.36108026,\n",
       " 0.3604297,\n",
       " 0.36176455,\n",
       " 0.35889554,\n",
       " 0.36064482,\n",
       " 0.3619299,\n",
       " 0.3608777,\n",
       " 0.3577779,\n",
       " 0.36006743,\n",
       " 0.36096963,\n",
       " 0.3598141,\n",
       " 0.36201718,\n",
       " 0.36145192,\n",
       " 0.36034325,\n",
       " 0.3614794,\n",
       " 0.36108264,\n",
       " 0.3605692,\n",
       " 0.36108705,\n",
       " 0.36188647,\n",
       " 0.36028007,\n",
       " 0.35972956,\n",
       " 0.36026812,\n",
       " 0.3606355,\n",
       " 0.3620454,\n",
       " 0.35904506,\n",
       " 0.36033726,\n",
       " 0.36027443,\n",
       " 0.36106014,\n",
       " 0.36167452,\n",
       " 0.361473,\n",
       " 0.35970047,\n",
       " 0.35946974,\n",
       " 0.3610926,\n",
       " 0.36194602,\n",
       " 0.36003092,\n",
       " 0.36006215,\n",
       " 0.3613809,\n",
       " 0.35944402,\n",
       " 0.359638,\n",
       " 0.36043975,\n",
       " 0.35920748,\n",
       " 0.36155817,\n",
       " 0.35975957,\n",
       " 0.3602578,\n",
       " 0.36193037,\n",
       " 0.35845223,\n",
       " 0.36147553,\n",
       " 0.36065182,\n",
       " 0.36230257,\n",
       " 0.36249998,\n",
       " 0.36131155,\n",
       " 0.3612271,\n",
       " 0.36021206,\n",
       " 0.35839036,\n",
       " 0.35826516,\n",
       " 0.35969856,\n",
       " 0.36082742,\n",
       " 0.36014915,\n",
       " 0.3608285,\n",
       " 0.35915413,\n",
       " 0.36019102,\n",
       " 0.36073747,\n",
       " 0.3595922,\n",
       " 0.36126274,\n",
       " 0.3598493,\n",
       " 0.3616805,\n",
       " 0.36093032,\n",
       " 0.36015037,\n",
       " 0.361861,\n",
       " 0.36133018,\n",
       " 0.35950744,\n",
       " 0.36186436,\n",
       " 0.36121848,\n",
       " 0.3619112,\n",
       " 0.36050513,\n",
       " 0.36126855,\n",
       " 0.3611028,\n",
       " 0.3606726,\n",
       " 0.36081624,\n",
       " 0.3599399,\n",
       " 0.35952622,\n",
       " 0.35886884,\n",
       " 0.35961375,\n",
       " 0.36111355,\n",
       " 0.36026174,\n",
       " 0.36133745,\n",
       " 0.360338,\n",
       " 0.36022955,\n",
       " 0.3597199,\n",
       " 0.36080518,\n",
       " 0.36034146,\n",
       " 0.36067048,\n",
       " 0.3618006,\n",
       " 0.3605226,\n",
       " 0.36253396,\n",
       " 0.35906282,\n",
       " 0.35901856,\n",
       " 0.36114395,\n",
       " 0.3620826,\n",
       " 0.3600902,\n",
       " 0.3609608,\n",
       " 0.36094236,\n",
       " 0.36111465,\n",
       " 0.36157966,\n",
       " 0.35995534,\n",
       " 0.36028796,\n",
       " 0.35979503,\n",
       " 0.3605834,\n",
       " 0.35823345,\n",
       " 0.35984126,\n",
       " 0.3607909,\n",
       " 0.36028436,\n",
       " 0.36132944,\n",
       " 0.36073574,\n",
       " 0.3599856,\n",
       " 0.3610736,\n",
       " 0.36185926,\n",
       " 0.3605314,\n",
       " 0.36025634,\n",
       " 0.35983202,\n",
       " 0.36087757,\n",
       " 0.3620268,\n",
       " 0.36067122,\n",
       " 0.3587016,\n",
       " 0.3609177,\n",
       " 0.35935342,\n",
       " 0.36012965,\n",
       " 0.3617666,\n",
       " 0.36091354,\n",
       " 0.36112833,\n",
       " 0.35886914,\n",
       " 0.36214456,\n",
       " 0.3603217,\n",
       " 0.35915303,\n",
       " 0.3617409,\n",
       " 0.36087516,\n",
       " 0.3579162,\n",
       " 0.35870615,\n",
       " 0.36040512,\n",
       " 0.3611644,\n",
       " 0.3606866,\n",
       " 0.3625811,\n",
       " 0.36134443,\n",
       " 0.3598826,\n",
       " 0.3603441,\n",
       " 0.35907272,\n",
       " 0.36056525,\n",
       " 0.36031076,\n",
       " 0.3614308,\n",
       " 0.3612744,\n",
       " 0.36176506,\n",
       " 0.36189935,\n",
       " 0.36179277,\n",
       " 0.3615749,\n",
       " 0.36215958,\n",
       " 0.3605648,\n",
       " 0.35960174,\n",
       " 0.36011055,\n",
       " 0.36153093,\n",
       " 0.36227566,\n",
       " 0.35923913,\n",
       " 0.35935017,\n",
       " 0.36125144,\n",
       " 0.35875657,\n",
       " 0.3609005,\n",
       " 0.361705,\n",
       " 0.36019954,\n",
       " 0.35801414,\n",
       " 0.3599873,\n",
       " 0.3592583,\n",
       " 0.360506,\n",
       " 0.35989898,\n",
       " 0.3611782,\n",
       " 0.3577323,\n",
       " 0.36121634,\n",
       " 0.36076465,\n",
       " 0.36202303,\n",
       " 0.36163178,\n",
       " 0.3600722,\n",
       " 0.35860154,\n",
       " 0.3614093,\n",
       " 0.3608677,\n",
       " 0.36152765,\n",
       " 0.3618856,\n",
       " 0.3610437,\n",
       " 0.36200753,\n",
       " 0.36027846,\n",
       " 0.36221513,\n",
       " 0.3632173,\n",
       " 0.36299914,\n",
       " 0.36313066,\n",
       " 0.3618864,\n",
       " 0.36186337,\n",
       " 0.361156,\n",
       " 0.35951462,\n",
       " 0.3597549,\n",
       " 0.36067143,\n",
       " 0.35963586,\n",
       " 0.35978898,\n",
       " 0.35942954,\n",
       " 0.36107534,\n",
       " 0.35922062,\n",
       " 0.36094046,\n",
       " 0.36219975,\n",
       " 0.36119846,\n",
       " 0.3606126,\n",
       " 0.36086467,\n",
       " 0.36039564,\n",
       " 0.36061597,\n",
       " 0.36005566,\n",
       " 0.36040008,\n",
       " 0.35958225,\n",
       " 0.3597544,\n",
       " 0.36115122,\n",
       " 0.35782087,\n",
       " 0.3598677,\n",
       " 0.36072737,\n",
       " 0.35945022,\n",
       " 0.3606419,\n",
       " 0.3602907,\n",
       " 0.3616944,\n",
       " 0.3612984,\n",
       " 0.36131024,\n",
       " 0.3619046,\n",
       " 0.3606252,\n",
       " 0.35974374,\n",
       " 0.36161333,\n",
       " 0.36176997,\n",
       " 0.36142868,\n",
       " 0.36128965,\n",
       " 0.3592522,\n",
       " 0.3587571,\n",
       " 0.36133564,\n",
       " 0.35942906,\n",
       " 0.36094397,\n",
       " 0.3614959,\n",
       " 0.36033332,\n",
       " 0.3607779,\n",
       " 0.36038703,\n",
       " 0.3612734,\n",
       " 0.3622086,\n",
       " 0.36185855,\n",
       " 0.3608042,\n",
       " 0.36090606,\n",
       " 0.3614442,\n",
       " 0.3613387,\n",
       " 0.360318,\n",
       " 0.36012614,\n",
       " 0.36210367,\n",
       " 0.3585686,\n",
       " 0.35793006,\n",
       " 0.36078656,\n",
       " 0.36034572,\n",
       " 0.3607732,\n",
       " 0.36159363,\n",
       " 0.35909283,\n",
       " 0.36125672,\n",
       " 0.3612003,\n",
       " 0.35955384,\n",
       " 0.36010548,\n",
       " 0.36013448,\n",
       " 0.36071712,\n",
       " 0.36093858,\n",
       " 0.35996848,\n",
       " 0.36194775,\n",
       " 0.36175928,\n",
       " 0.3593489,\n",
       " 0.36134243,\n",
       " 0.35917062,\n",
       " 0.36043164,\n",
       " 0.36198464,\n",
       " 0.3613207,\n",
       " 0.36167812,\n",
       " 0.3602007,\n",
       " 0.36024454,\n",
       " 0.36085528,\n",
       " 0.3621934,\n",
       " 0.36126637,\n",
       " 0.3605638,\n",
       " 0.36159676,\n",
       " 0.36059284,\n",
       " 0.3599527,\n",
       " 0.361628,\n",
       " 0.36054918,\n",
       " 0.3592209,\n",
       " 0.35924736,\n",
       " 0.35984644,\n",
       " 0.35951072,\n",
       " 0.35883677,\n",
       " 0.36220098,\n",
       " 0.3621712,\n",
       " 0.3600558,\n",
       " 0.36065167,\n",
       " 0.35997972,\n",
       " 0.360735,\n",
       " 0.36018166,\n",
       " 0.3608308,\n",
       " 0.3600539,\n",
       " 0.36060825,\n",
       " 0.36140618,\n",
       " 0.36184785,\n",
       " 0.35787967,\n",
       " 0.35921928,\n",
       " 0.3597796,\n",
       " 0.36097804,\n",
       " 0.36172265,\n",
       " 0.36172706,\n",
       " 0.36042917,\n",
       " 0.36201623,\n",
       " 0.3601002,\n",
       " 0.36068252,\n",
       " 0.36041704,\n",
       " 0.359438,\n",
       " 0.36213008,\n",
       " 0.36138353,\n",
       " 0.36020428,\n",
       " 0.36017329,\n",
       " 0.3603684,\n",
       " 0.36167827,\n",
       " 0.3585409,\n",
       " 0.3602042,\n",
       " 0.3591187,\n",
       " 0.36042222,\n",
       " 0.36187193,\n",
       " 0.3603267,\n",
       " 0.35816368,\n",
       " 0.36009938,\n",
       " 0.3613395,\n",
       " 0.35949004,\n",
       " 0.3619709,\n",
       " 0.35819477,\n",
       " 0.36009622,\n",
       " 0.3595506,\n",
       " 0.35985988,\n",
       " 0.36141583,\n",
       " 0.36045587,\n",
       " 0.36172086,\n",
       " 0.3603656,\n",
       " 0.36068976,\n",
       " 0.36217058,\n",
       " 0.36230585,\n",
       " 0.36176515,\n",
       " 0.36222515,\n",
       " 0.36184493,\n",
       " 0.36166418,\n",
       " 0.36207822,\n",
       " 0.36015594,\n",
       " 0.36130476,\n",
       " 0.3599284,\n",
       " 0.3612863,\n",
       " 0.35811478,\n",
       " 0.35941094,\n",
       " 0.35999045,\n",
       " 0.36052164,\n",
       " 0.36087102,\n",
       " 0.36070037,\n",
       " 0.36096913,\n",
       " 0.3613306,\n",
       " 0.36214274,\n",
       " 0.35903051,\n",
       " 0.3598803,\n",
       " 0.3597237,\n",
       " 0.3613996,\n",
       " 0.36047024,\n",
       " 0.3601685,\n",
       " 0.36115465,\n",
       " 0.36222157,\n",
       " 0.36042893,\n",
       " 0.36237985,\n",
       " 0.36111426,\n",
       " 0.35951063,\n",
       " 0.35993272,\n",
       " 0.36157304,\n",
       " 0.36138383,\n",
       " 0.36119115,\n",
       " 0.35886925,\n",
       " 0.36036128,\n",
       " 0.35982075,\n",
       " 0.36083692,\n",
       " 0.36183527,\n",
       " 0.36001942,\n",
       " 0.3610107,\n",
       " 0.3615836,\n",
       " 0.3596958,\n",
       " 0.3602322,\n",
       " 0.3594261,\n",
       " 0.36084777,\n",
       " 0.36167422,\n",
       " 0.36085823,\n",
       " 0.36057237,\n",
       " 0.3614201,\n",
       " 0.36035675,\n",
       " 0.3606104,\n",
       " 0.3604015,\n",
       " 0.35872668,\n",
       " 0.3599226,\n",
       " 0.35923922,\n",
       " 0.3616958,\n",
       " 0.3611806,\n",
       " 0.36020258,\n",
       " 0.36119518,\n",
       " 0.3614673,\n",
       " 0.36161664,\n",
       " 0.36193055,\n",
       " 0.35902846,\n",
       " 0.36127603,\n",
       " 0.35888958,\n",
       " 0.3600628,\n",
       " 0.36054817,\n",
       " 0.360164,\n",
       " 0.36214155,\n",
       " 0.36144674,\n",
       " 0.3596543,\n",
       " 0.35992217,\n",
       " 0.3577379,\n",
       " 0.35930908,\n",
       " 0.36079335,\n",
       " 0.36161217,\n",
       " 0.36159867,\n",
       " 0.35796347,\n",
       " 0.36034152,\n",
       " 0.36141354,\n",
       " 0.35838598,\n",
       " 0.36025923,\n",
       " 0.36122406,\n",
       " 0.36130258,\n",
       " 0.3594501,\n",
       " 0.3592789,\n",
       " 0.3593176,\n",
       " 0.36132786,\n",
       " 0.3616528,\n",
       " 0.35883617,\n",
       " 0.3614006,\n",
       " 0.3605531,\n",
       " 0.36110392,\n",
       " 0.36075497,\n",
       " 0.36015743,\n",
       " 0.3607011,\n",
       " 0.36013985,\n",
       " 0.3613171,\n",
       " 0.36119792,\n",
       " 0.35948414,\n",
       " 0.3594467,\n",
       " 0.3611208,\n",
       " 0.36032864,\n",
       " 0.36048415,\n",
       " 0.3601599,\n",
       " 0.36163118,\n",
       " 0.36207283,\n",
       " 0.35913974,\n",
       " 0.35932934,\n",
       " 0.3608588,\n",
       " 0.3613666,\n",
       " 0.36089516,\n",
       " 0.35940936,\n",
       " 0.36093584,\n",
       " 0.35991985,\n",
       " 0.3610871,\n",
       " 0.36157656,\n",
       " 0.3606952,\n",
       " 0.35988438,\n",
       " 0.36125702,\n",
       " 0.36232772,\n",
       " 0.3613195,\n",
       " 0.3621463,\n",
       " 0.36219928,\n",
       " 0.36022174,\n",
       " 0.3610316,\n",
       " 0.3603302,\n",
       " 0.36036158,\n",
       " 0.36167866,\n",
       " 0.3601403,\n",
       " 0.3615639,\n",
       " 0.36185893,\n",
       " 0.36102337,\n",
       " 0.3607786,\n",
       " 0.36122927,\n",
       " 0.36164746,\n",
       " 0.3613778,\n",
       " 0.36136296,\n",
       " 0.36219186,\n",
       " 0.36105788,\n",
       " 0.35957554,\n",
       " 0.3597077,\n",
       " 0.36183396,\n",
       " 0.36042956,\n",
       " 0.36013308,\n",
       " 0.36029822,\n",
       " 0.36114824,\n",
       " 0.36120036,\n",
       " 0.36012867,\n",
       " 0.36056554,\n",
       " 0.36071223,\n",
       " 0.36039025,\n",
       " 0.3589569,\n",
       " 0.36044946,\n",
       " 0.36002627,\n",
       " 0.36049756,\n",
       " 0.35999593,\n",
       " 0.36106238,\n",
       " 0.3616329,\n",
       " 0.36112764,\n",
       " 0.3603046,\n",
       " 0.35944894,\n",
       " 0.36011636,\n",
       " 0.36017016,\n",
       " 0.36076623,\n",
       " 0.36008608,\n",
       " 0.35948184,\n",
       " 0.36135915,\n",
       " 0.36160386,\n",
       " 0.3613948,\n",
       " 0.36150494,\n",
       " 0.35898125,\n",
       " 0.35849428,\n",
       " 0.3612453,\n",
       " 0.36043257,\n",
       " 0.35998,\n",
       " 0.35858303,\n",
       " 0.36206856,\n",
       " 0.3616333,\n",
       " 0.36058527,\n",
       " 0.3616905,\n",
       " 0.36077943,\n",
       " 0.3611919,\n",
       " 0.360013,\n",
       " 0.36011666,\n",
       " 0.3613027,\n",
       " 0.36031613,\n",
       " 0.36202848,\n",
       " 0.36117,\n",
       " 0.35952607,\n",
       " 0.35962117,\n",
       " 0.36034104,\n",
       " 0.361575,\n",
       " 0.36107406,\n",
       " 0.36062762,\n",
       " 0.36121085,\n",
       " 0.36046815,\n",
       " 0.3600237,\n",
       " 0.36184102,\n",
       " 0.3610397,\n",
       " 0.3601332,\n",
       " 0.36104316,\n",
       " 0.36158615,\n",
       " 0.36054936,\n",
       " 0.36023575,\n",
       " 0.36049002,\n",
       " 0.3578622,\n",
       " 0.3602418,\n",
       " 0.36106658,\n",
       " 0.3599801,\n",
       " 0.36203235,\n",
       " 0.36065912,\n",
       " 0.36015636,\n",
       " 0.360676,\n",
       " 0.36004376,\n",
       " 0.36070117,\n",
       " 0.36074543,\n",
       " 0.35992423,\n",
       " 0.36057645,\n",
       " 0.36056975,\n",
       " 0.3614978,\n",
       " 0.36001727,\n",
       " 0.36138302,\n",
       " 0.3617726,\n",
       " 0.3606796,\n",
       " 0.36103365,\n",
       " 0.35839453,\n",
       " 0.3594721,\n",
       " 0.36062613,\n",
       " 0.36016175,\n",
       " 0.36132002,\n",
       " 0.36022538,\n",
       " 0.36251646,\n",
       " 0.36168975,\n",
       " 0.36223826,\n",
       " 0.3614404,\n",
       " 0.35930002,\n",
       " 0.36194244,\n",
       " 0.36150876,\n",
       " 0.36014077,\n",
       " 0.35940495,\n",
       " 0.360627,\n",
       " 0.36074474,\n",
       " 0.3622972,\n",
       " 0.3617431,\n",
       " 0.36071968,\n",
       " 0.36041477,\n",
       " 0.36017302,\n",
       " 0.3599029,\n",
       " 0.35990587,\n",
       " 0.36148942,\n",
       " 0.36045602,\n",
       " 0.36188117,\n",
       " 0.3609221,\n",
       " 0.36017436,\n",
       " 0.3612331,\n",
       " 0.3619681,\n",
       " 0.3621432,\n",
       " 0.36183727,\n",
       " 0.3602359,\n",
       " 0.3611356,\n",
       " 0.35962248,\n",
       " 0.35998183,\n",
       " 0.36061946,\n",
       " 0.36023512,\n",
       " 0.35840088,\n",
       " 0.36025363,\n",
       " 0.36011216,\n",
       " 0.36090395,\n",
       " 0.36149272,\n",
       " 0.36135206,\n",
       " 0.36201277,\n",
       " 0.36089885,\n",
       " 0.36028767,\n",
       " 0.36031824,\n",
       " 0.36097267,\n",
       " 0.36059886,\n",
       " 0.3605596,\n",
       " 0.35876963,\n",
       " 0.36131692,\n",
       " 0.36186236,\n",
       " 0.35937774,\n",
       " 0.36098745,\n",
       " 0.3608837,\n",
       " 0.36007047,\n",
       " 0.36072946,\n",
       " 0.36198372,\n",
       " 0.36020166,\n",
       " 0.36108068,\n",
       " 0.36202908,\n",
       " 0.36196738,\n",
       " 0.3621879,\n",
       " 0.36147067,\n",
       " 0.35987246,\n",
       " 0.35945448,\n",
       " 0.3604424,\n",
       " 0.360486,\n",
       " 0.35990196,\n",
       " 0.3610545,\n",
       " 0.36162853,\n",
       " 0.36065602,\n",
       " 0.35970154,\n",
       " 0.36153,\n",
       " 0.36214343,\n",
       " 0.35866177,\n",
       " 0.35996783,\n",
       " 0.36089858,\n",
       " 0.3590922,\n",
       " 0.36000502,\n",
       " 0.36112744,\n",
       " 0.35872582,\n",
       " 0.36203316,\n",
       " 0.36008248,\n",
       " 0.36145708,\n",
       " 0.36180088,\n",
       " 0.36142802,\n",
       " 0.36001688,\n",
       " 0.3606233,\n",
       " 0.360187,\n",
       " 0.3599296,\n",
       " 0.36125752,\n",
       " 0.3606264,\n",
       " 0.3617625,\n",
       " 0.36189982,\n",
       " 0.3614878,\n",
       " 0.36016867,\n",
       " 0.36061254,\n",
       " 0.360853,\n",
       " 0.3585905,\n",
       " 0.36054787,\n",
       " 0.36191982,\n",
       " 0.36169904,\n",
       " 0.35983267,\n",
       " 0.35914227,\n",
       " 0.36186883,\n",
       " 0.36071408,\n",
       " 0.3601529,\n",
       " 0.3606364,\n",
       " 0.3600942,\n",
       " 0.36212718,\n",
       " 0.36104637,\n",
       " 0.36080867,\n",
       " 0.360469,\n",
       " 0.3596243,\n",
       " 0.3605792,\n",
       " 0.3607155,\n",
       " 0.36146393,\n",
       " 0.35851932,\n",
       " 0.361642,\n",
       " 0.3613379,\n",
       " 0.35936952,\n",
       " 0.359786,\n",
       " 0.3609351,\n",
       " 0.35909984,\n",
       " 0.36080483,\n",
       " 0.3622268,\n",
       " 0.359719,\n",
       " 0.36076814,\n",
       " 0.3604484,\n",
       " 0.35929248,\n",
       " 0.36109614,\n",
       " 0.3601582,\n",
       " 0.36050326,\n",
       " 0.3608284,\n",
       " 0.36037666,\n",
       " 0.3573389,\n",
       " 0.3607438,\n",
       " 0.36185306,\n",
       " 0.36161634,\n",
       " 0.3602761,\n",
       " 0.36046168,\n",
       " 0.361816,\n",
       " 0.36203173,\n",
       " 0.3614556,\n",
       " 0.36219394,\n",
       " 0.35934454,\n",
       " 0.36138278,\n",
       " 0.3591136,\n",
       " 0.35920557,\n",
       " 0.36132756,\n",
       " 0.36010775,\n",
       " 0.3609518,\n",
       " 0.3611761,\n",
       " 0.35771865,\n",
       " 0.36084703,\n",
       " 0.36003307,\n",
       " 0.36078948,\n",
       " 0.36064714,\n",
       " 0.3604596,\n",
       " 0.36067593,\n",
       " 0.36010993,\n",
       " 0.36173373,\n",
       " 0.3619633,\n",
       " 0.36167416,\n",
       " 0.36133605,\n",
       " 0.36184475,\n",
       " 0.36214733,\n",
       " 0.3619107,\n",
       " 0.3599186,\n",
       " 0.36079907,\n",
       " 0.3582779,\n",
       " 0.36141434,\n",
       " 0.3592562,\n",
       " 0.3608202,\n",
       " 0.36155292,\n",
       " 0.36062786,\n",
       " 0.3595526,\n",
       " 0.36003104,\n",
       " 0.35903618,\n",
       " 0.36041218,\n",
       " 0.35966003,\n",
       " 0.36059114,\n",
       " 0.3610877,\n",
       " 0.36073914,\n",
       " 0.36032644,\n",
       " 0.36188397,\n",
       " 0.35973436,\n",
       " 0.35930625,\n",
       " 0.36162388,\n",
       " 0.36175293,\n",
       " 0.36224294,\n",
       " 0.35908496,\n",
       " 0.36118925,\n",
       " 0.35817558,\n",
       " 0.3592865,\n",
       " 0.35942215,\n",
       " 0.36027655,\n",
       " 0.357982,\n",
       " 0.3603433,\n",
       " 0.36068535,\n",
       " 0.36220148,\n",
       " 0.36200827,\n",
       " 0.36161852,\n",
       " 0.36216477,\n",
       " 0.3614834,\n",
       " 0.35996634,\n",
       " 0.35931683,\n",
       " 0.35888624,\n",
       " 0.36215538,\n",
       " 0.35829324,\n",
       " 0.36075237,\n",
       " 0.36091253,\n",
       " 0.36123678,\n",
       " 0.36144033,\n",
       " 0.36086035,\n",
       " 0.35822567,\n",
       " 0.35846794,\n",
       " 0.36074165,\n",
       " 0.36004516,\n",
       " 0.35980076,\n",
       " 0.36089742,\n",
       " 0.36192477,\n",
       " 0.36042726,\n",
       " 0.36046094,\n",
       " 0.36044014,\n",
       " 0.36161283,\n",
       " 0.3625839,\n",
       " 0.36002412,\n",
       " 0.3606077,\n",
       " 0.3603804,\n",
       " 0.36074096,\n",
       " 0.36045995,\n",
       " 0.3602173,\n",
       " 0.3602176,\n",
       " 0.3617402,\n",
       " 0.3606149,\n",
       " 0.36025932,\n",
       " 0.36099014,\n",
       " 0.36085144,\n",
       " 0.3602605,\n",
       " 0.36034814,\n",
       " 0.35982934,\n",
       " 0.3612082,\n",
       " 0.36150047,\n",
       " 0.36214465,\n",
       " 0.35906583,\n",
       " 0.360515,\n",
       " 0.36068344,\n",
       " 0.36014834,\n",
       " 0.36023208,\n",
       " 0.36076218,\n",
       " 0.36062554,\n",
       " 0.36010733,\n",
       " 0.3587834,\n",
       " 0.3609872,\n",
       " 0.36085972,\n",
       " 0.35870096,\n",
       " 0.35954636,\n",
       " 0.35880578,\n",
       " 0.36056682,\n",
       " 0.36117417,\n",
       " 0.36159152,\n",
       " 0.3610324,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_true1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_true1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsna3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ebb975f138c097a137a6dc12a4a39951a694257232e4650cfcbe763fc36433c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
